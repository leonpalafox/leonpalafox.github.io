---
layout: single
title: Los datos representan el mundo, y los modelos aprenden de los datos [Spanish]
toc: False
author_profile: True
category: ['articulos']
tags: [ 'mexico', 'financiero', 'spanish']
---

Nota: Artículo publicado en [El Financiero](https://www.elfinanciero.com.mx/opinion/leon-palafox/los-datos-representan-el-mundo-y-los-modelos-aprenden-de-los-datos)

En el 2015 un tweet, hoy borrado, indicó la forma en la cual los modelos de Inteligencia Artificial clasificaban como gorila a una persona de piel oscura. Así, varios análisis han descubierto cómo las decisiones tomadas usando modelos de IA tienden a reforzar o incrementar sesgos que existen actualmente. Desde desviaciones debido al género hasta el nombre. Ha habido múltiples casos donde sistemas automatizados han tomado decisiones favorecedoras para un grupo en particular. Un reciente caso famoso es el entrevistador automatizado que recomendaba contratar más hombres que mujeres en Amazon.

Dados todos estos problemas de sesgo en la IA, hoy hay un debate rugiendo en el centro de la comunidad de desarrolladores e investigadores. Este debate trata de responder la pregunta: ¿Quién es la parte responsable de supervisar y corregir estos algoritmos para evitar estas decisiones sesgadas? El debate resuelve que puede haber dos partes responsables: por un lado, los diseñadores de los modelos, y por el otro, los datos mismos usados en los modelos.

Tenemos a un grupo de expertos argumentando que los datos, desde su incepción, están sesgados, y estos representan las desviaciones y tendencias que tenemos nosotros los humanos. El trabajo de los modelos es replicar el comportamiento de estos datos de la manera más fidedigna posible. Entonces, los investigadores y diseñadores deben darse a la tarea de buscar datos que no tengan estos sesgos y tendencias, y no, en modificar sus modelos.

Este tipo de pensar remueve toda responsabilidad de la persona que diseña el modelo y la centra en la recolección de datos. Si bien esta forma permite desarrollos más rápidos y ágiles, termina resultando en algo muy cómodo simplemente justificarse en que los datos están mal y no buscar soluciones que coadyuven la mitigación de la problemática.

Del otro lado, tenemos al otro grupo de expertos que propone que las personas a cargo de diseñar los modelos son los encargados de verificar que los modelos no generen resultados sesgados. Que, independientemente de lo tendenciosos de los datos, los modelos mismos deben de ser capaces de extraer dichos sesgos y corregirlos para no cometer la toma de decisiones erróneas. Al igual que con el grupo anterior, donde los datos eran el problema, éste grupo opina que la responsabilidad, no recae en la recopilación de datos, o la existencia nativa de sesgos, sino en los modelos mismos, y hace que toda la responsabilidad recaiga en la persona o grupo diseñador del modelo. De igual manera, uno podría argumentar que al remover los sesgos, nos volvemos ciegos a los problemas y solo estamos rehaciendo el mundo como queremos que sea, en lugar de aceptarlo como es.

Ambas aproximaciones tienen sus ventajas y desventajas, si la culpa fuese sólo de los datos, estaríamos siempre esperando que los datos no fueran sesgados, sin tratar de generar algún cambio interno, estaríamos constantemente justificándonos en que el mundo es así, sin que nuestro modelo lo intente resolver. Por el otro lado, si la culpa es de los modelos, y la diseñadora está encargada de remover sesgos: ¿Qué va a evitar que los propios sesgos e ideologías de la diseñadora influencien el resultado del modelo y cómo el resultado afecte el negocio directamente?

Es fácil encontrar ejemplos que justifiquen ambas ideologías: Pensemos en los créditos bancarios, donde si un modelo es entrenado con datos que contienen el color de piel, podemos tener casos donde personas de piel más oscura han incurrido en morosidad constantemente.

Si utilizamos dichos datos en un país donde la mayoría de las personas tiene la piel oscura, jamás se otorgaría un crédito y tendríamos que cerrar el banco. Por el otro lado; si eliminamos ese sesgo, quizás si se otorgasen muchos créditos a personas que, efectivamente, no pueden pagarlos, perdiendo de nuevo dinero y resultando en algo negativo para el banco.

En mi opinión debemos tener un justo medio, donde el diseñador trate de usar datos lo menos sesgados posibles, pero al mismo tiempo tratar de hacer un modelo que no incremente las discriminaciones que ya existen y de cierta manera, tratar de decrementarlas de alguna u otra forma. Quizás diseñando un crédito ajustado para personas que históricamente no han pagado a tiempo, lo cual resultaría en un beneficio neto para el banco y el cliente.

Al final del día, los modelos de IA no hacen más que replicar el mundo que existe ahí afuera y está en nosotros tratar de crear un mundo mejor.

# Noticas éticas

Google AI, la rama dedicada a la inteligencia artificial en Google está siendo fuertemente atacada por su decisión de terminar sus relaciones laborales con dos expertas en el campo de IA y Ética. Dicha decisión pone en evidencia que hoy en día, aún en algunas de las empresas más importantes de la tecnología, el tema de la ética en IA no se ha resuelto y requiere constante trabajo.